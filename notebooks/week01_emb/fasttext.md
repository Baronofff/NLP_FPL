**FastText** — это библиотека и алгоритм, разработанные исследователями из Facebook AI Research (FAIR) для создания векторных представлений слов (word embeddings). Основное отличие FastText от других моделей, таких как Word2Vec или GloVe, заключается в том, что FastText учитывает внутреннюю структуру слов (подслова), что позволяет модели лучше справляться с редкими и неизвестными словам. В этом ответе я подробно опишу алгоритм FastText.

---

## **1. Введение в FastText**

### **1.1. Основные идеи**
- **Подслова (subwords)**: FastText разбивает слова на подслова (например, префиксы, суффиксы и корни). Это позволяет модели "понимать" морфологию слов и обрабатывать новые или редкие слова.
- **Пример**: Слово "яблоко" может быть представлено через подслова ["я", "бл", "око"].
- **Обработка неизвестных слов**: Если слово не встречалось в обучающем корпусе, модель все равно сможет вычислить его эмбеддинг, используя подслова.

### **1.2. Преимущества FastText**
- Более высокое качество эмбеддингов для языков с богатой морфологией (например, русский язык).
- Возможность работы с новыми или редкими словами.
- Улучшенная интерпретируемость: можно анализировать, какие подслова влияют на эмбеддинг слова.

---

## **2. Архитектура FastText**

### **2.1. Принцип работы**
FastText использует две основные архитектуры:
- **CBOW (Continuous Bag of Words)**: Предсказание центрального слова по контексту.
- **Skip-gram**: Предсказание контекста по центральному слову.

Основное отличие FastText от Word2Vec заключается в том, что вместо обучения эмбеддинга для каждого слова FastText обучает эмбеддинги для подслов, которые затем используются для получения эмбеддинга целого слова.

### **2.2. Подслова**
Для каждого слова \( w \) строится набор подслов (n-грамм):
- Например, для слова "яблоко" могут быть использованы подслова ["<я", "яб", "бл", "лок", "око>", "<о>"], где символы "<" и ">" добавлены для обозначения начала и конца слова.
- Размер n-грамм обычно выбирается в диапазоне от 3 до 6 символов.

### **2.3. Эмбеддинг слова**
Эмбеддинг слова \( w \) в FastText получается как сумма эмбеддингов всех его подслов:
\[
v_w = \sum_{s \in S(w)} v_s
\]
Где:
- \( S(w) \) — множество подслов слова \( w \).
- \( v_s \) — эмбеддинг подслова \( s \).

---

## **3. Алгоритм обучения FastText**

### **3.1. Подготовка данных**
1. **Токенизация текста**: Текст разбивается на отдельные слова (токены).
2. **Построение словаря**: Создается список уникальных слов (словарь). Слова, которые встречаются реже определенного порога (`min_count`), исключаются.
3. **Извлечение подслов**: Для каждого слова из словаря извлекаются подслова (n-граммы).

### **3.2. Инициализация параметров**
1. **Эмбеддинги подслов**: Для каждого подслова создается случайный вектор длины `vector_size` (например, 300).
2. **Матрицы весов**: Для каждой архитектуры (CBOW или Skip-gram) создаются матрицы весов, которые будут обновляться во время обучения.

### **3.3. Процесс обучения**
1. **Выбор примера**: Выбирается центральное слово и его контекст.
2. **Вычисление эмбеддинга слова**:
   - Для выбранного слова \( w \) вычисляется его эмбеддинг как сумму эмбеддингов всех его подслов.
3. **Вычисление вероятностей**:
   - Для **CBOW**: Эмбеддинги всех контекстных слов суммируются, и на основе этой суммы вычисляется вероятность целевого слова.
   - Для **Skip-gram**: Эмбеддинг центрального слова используется для предсказания вероятности каждого контекстного слова.
4. **Вычисление ошибки**: Используется функция потерь (обычно это **softmax**), которая показывает, насколько хорошо модель предсказывает правильные слова.
5. **Обновление весов**: На основе ошибки обновляются веса (элементы матриц) с помощью метода обратного распространения ошибки (backpropagation).

### **3.4. Функция потерь**
Для вычисления вероятностей используется **softmax**:

\[
P(w_t \mid w_{t-c}, \dots, w_{t+c}) = \frac{\exp(u_{w_t}^T v_{w_c})}{\sum_{w' \in V} \exp(u_{w'}^T v_{w_c})}
\]

Где:
- \( u_{w_t} \) — эмбеддинг целевого слова.
- \( v_{w_c} \) — эмбеддинг контекстного слова.
- \( V \) — размер словаря.

### **3.5. Ускорение обучения**
Как и в Word2Vec, softmax вычисляется по всему словарю, что очень медленно для больших корпусов. Поэтому используются следующие методы ускорения:

1. **Negative Sampling**: Вместо того чтобы вычислять softmax для всего словаря, выбирается несколько "отрицательных" слов (слов, которые не являются контекстными) и обучается только на них.
   
   Формула для negative sampling:

   \[
   L = \log \sigma(u_{w_t}^T v_{w_c}) + \sum_{i=1}^{k} \log \sigma(-u_{w_i}^T v_{w_c})
   \]

   Где \( k \) — количество отрицательных выборок.

2. **Hierarchical Softmax**: Вместо вычисления softmax по всему словарю строится дерево Хаффмана, где каждый узел соответствует одному слову. Вероятность слова вычисляется путем перемещения по дереву.

---

## **4. Практика с FastText**

### **4.1. Пример кода на Python**
Вот пример кода на Python с использованием библиотеки `fasttext`:

```python
import fasttext

# Пример корпуса текста
corpus = [
    "the cat sat on the mat",
    "the dog sat on the log",
    "cats and dogs are friends"
]

# Сохраняем корпус в файл
with open("corpus.txt", "w") as f:
    for sentence in corpus:
        f.write(sentence + "\n")

# Обучение модели FastText (Skip-gram)
model = fasttext.train_unsupervised("corpus.txt", model='skipgram', dim=100, ws=5, minCount=1)

# Получение вектора для слова
print(model.get_word_vector("cat"))

# Поиск ближайших слов
print(model.get_nearest_neighbors("cat"))
```

### **4.2. Задачи для студентов**
- Попросите студентов изменить гиперпараметры (размер окна, размер вектора, минимальное количество встречаемости слов) и посмотреть, как это влияет на результат.
- Попросите их найти семантические аналогии, например, "король" — "мужчина" = "королева" — "женщина".
- Попробуйте использовать FastText для классификации текстов и сравните результаты с другими моделями.

---

## **5. Сравнение с другими моделями**

### **5.1. Word2Vec**
- **Word2Vec** работает с полными словами и не учитывает их внутреннюю структуру.
- **FastText** использует подслова, что позволяет ему лучше справляться с редкими и новыми словами.

### **5.2. GloVe**
- **GloVe** строит эмбеддинги на основе глобальных статистических отношений между словами.
- **FastText** фокусируется на локальном контексте и морфологии слов.

### **5.3. Преимущества FastText**
- Лучшее качество для языков с богатой морфологией.
- Возможность работы с новыми или редкими словами.
- Улучшенная интерпретируемость за счет использования подслов.

---

## **6. Заключение**

FastText — это мощный инструмент для создания векторных представлений слов, который особенно полезен для языков с богатой морфологией. Его способность учитывать подслова делает его более гибким и адаптивным к различным задачам. Понимание принципов работы FastText поможет вам эффективно применять эту модель в реальных задачах машинного обучения.
