
# Сравним модели   Word2Vec, GloVe, FastText

| Критерий                         | Word2Vec                                                                 | GloVe (Global Vectors)                                             | FastText                                                                 |
|----------------------------------|--------------------------------------------------------------------------|---------------------------------------------------------------------|-------------------------------------------------------------------------|
| **Тип модели**                   | Нейронная сеть, обучаемая на основе контекста слов                       | Матричная факторизация совместных вероятностей встречаемости слов   | Нейронная сеть с учетом подслов (subwords)                              |
| **Архитектура**                  | CBOW (Continuous Bag of Words) или Skip-gram                             | Строится на основе матрицы совместной встречаемости слов            | Похожа на Word2Vec, но добавлен слой для работы с подсловами           |
| **Обработка многозначности**     | Не учитывает многозначность слов                                         | Не учитывает многозначность слов                                     | Учитывает многозначность через подслова                                 |
| **Работа с неизвестными словам** | Игнорирует слова, которые не встречаются в обучающем корпусе             | Тоже игнорирует слова, которых нет в словаре                        | Обрабатывает новые слова через подслова                                 |
| **Скорость обучения**            | Зависит от размера корпуса и гиперпараметров, может быть медленным       | Часто быстрее, чем Word2Vec, так как использует предварительно построенную матрицу | Быстрее Word2Vec за счет использования подслов                         |
| **Качество эмбеддингов**         | Хорошее качество для общих задач, но может теряться в специализированных корпусах | Лучше работает для семантических аналогий благодаря глобальной структуре | Высокое качество из-за способности модели учитывать морфологию слов    |
| **Поддержка языков**             | Поддерживает только полные слова                                        | Поддерживает только полные слова                                     | Поддерживает подслова (например, "я", "блю" для слова "яблоко")        |
| **Моделируемая информация**      | Контекстное окружение слов                                               | Глобальные статистические отношения между словами                    | Комбинирует локальный контекст с морфологическими особенностями слов    |
| **Пример использования**         | Определение синонимов, рекомендация контента                             | Анализ текстов с акцентом на семантические связи                     | Обработка редких слов, работа с языками с богатой морфологией          |
| **Особенности реализации**       | Реализован в библиотеке `gensim` и других инструментах                   | Реализован в библиотеке `glove-python` и других                      | Реализован в библиотеке `fasttext`, разработанной Facebook Research    |
| **Основные плюсы**               | Простота использования, хорошее качество эмбеддингов для общего корпуса   | Линейная алгебра позволяет строить более стабильные модели           | Учет морфологии слов, высокая адаптивность к новым словам              |
| **Основные минусы**              | Не учитывает многозначность слов, проблемы с новыми словам               | Не учитывает контекстные особенности слов                           | Может потребовать больше вычислительных ресурсов для больших корпусов  |


Метод **обратного распространения ошибки (backpropagation)** — это ключевой алгоритм для обучения нейронных сетей. Он позволяет корректировать веса сети, чтобы минимизировать разницу между предсказанными и реальными значениями. В этом ответе я подробно опишу, как работает backpropagation, шаг за шагом.

---

## **1. Введение в обратное распространение ошибки**

### **1.1. Что такое обратное распространение?**
Backpropagation — это метод, который используется для обновления весов в нейронной сети на основе вычисленной ошибки. Основная идея заключается в том, что ошибка "распространяется" от выходного слоя к входному, а веса каждого слоя корректируются по мере продвижения назад.

### **1.2. Почему нужен backpropagation?**
- Нейронные сети состоят из множества слоев, каждый из которых имеет свои веса.
- Для того чтобы сеть правильно обучалась, необходимо изменять эти веса так, чтобы уменьшить разницу между предсказанными и целевыми значениями (функция потерь).
- Backpropagation помогает эффективно вычислять градиенты функции потерь относительно всех весов сети.

---

## **2. Математические основы backpropagation**

### **2.1. Функция потерь**
Функция потерь \( L \) измеряет, насколько сильно предсказанные значения отличаются от истинных значений. Общая формула:

\[
L = \frac{1}{N} \sum_{i=1}^{N} \text{loss}(y_i, \hat{y}_i)
\]

Где:
- \( N \) — количество примеров в тренировочном наборе данных.
- \( y_i \) — истинное значение для i-го примера.
- \( \hat{y}_i \) — предсказанное значение для i-го примера.
- \( \text{loss}(\cdot) \) — функция потерь, например, среднеквадратичная ошибка (MSE) или кросс-энтропия.

### **2.2. Градиентный спуск**
Для минимизации функции потерь используется **градиентный спуск**. Основная идея:
- Вычислите градиент функции потерь по всем весам сети.
- Изменяйте веса в направлении, противоположном градиенту, чтобы уменьшить значение функции потерь.

Формально:
\[
w := w - \eta \nabla_w L
\]

Где:
- \( w \) — веса сети.
- \( \eta \) — скорость обучения (learning rate).
- \( \nabla_w L \) — градиент функции потерь по весам \( w \).

---

## **3. Алгоритм обратного распространения ошибки**

### **3.1. Шаг 1: Прямое распространение (Forward Pass)**
Прежде чем рассчитывать градиенты, нужно выполнить прямое распространение через сеть, чтобы получить предсказания.

#### **Шаги:**
1. Инициализируйте входные данные \( x \).
2. Проходите через каждый слой сети:
   - Вычислите активации текущего слоя на основе входных данных и весов.
   - Примените функцию активации (например, ReLU, sigmoid, softmax).
3. Получите выходное значение \( \hat{y} \).

Формула для одного слоя:
\[
z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}
\]
\[
a^{(l)} = f(z^{(l)})
\]

Где:
- \( z^{(l)} \) — линейная комбинация входных данных и весов на l-м слое.
- \( W^{(l)} \) — матрица весов l-го слоя.
- \( b^{(l)} \) — вектор смещений (biases) l-го слоя.
- \( a^{(l)} \) — активации l-го слоя после применения функции активации \( f \).

### **3.2. Шаг 2: Вычисление ошибки (Loss Calculation)**
После прямого распространения вычислите функцию потерь \( L \), сравнив предсказанные значения \( \hat{y} \) с истинными значениями \( y \).

### **3.3. Шаг 3: Обратное распространение (Backward Pass)**
Теперь нужно "развернуть" процесс и вычислить градиенты функции потерь по всем весам сети.

#### **Шаги:**
1. **Вычисление градиента на последнем слое**:
   - Найдите производную функции потерь по выходным активациям последнего слоя:
     \[
     \delta^{(L)} = \frac{\partial L}{\partial a^{(L)}} \odot f'(z^{(L)})
     \]
     Где \( f' \) — производная функции активации, а \( \odot \) — поэлементное произведение.
   
2. **Рекурсивное вычисление градиентов для предыдущих слоев**:
   - Для каждого слоя \( l \) от \( L-1 \) до 1:
     \[
     \delta^{(l)} = \left( W^{(l+1)} \right)^T \delta^{(l+1)} \odot f'(z^{(l)})
     \]
     Где \( \left( W^{(l+1)} \right)^T \) — транспонированная матрица весов следующего слоя.

3. **Вычисление градиентов по весам и смещениям**:
   - Для каждого слоя \( l \):
     \[
     \frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} \left( a^{(l-1)} \right)^T
     \]
     \[
     \frac{\partial L}{\partial b^{(l)}} = \delta^{(l)}
     \]

### **3.4. Шаг 4: Обновление весов**
Используйте вычисленные градиенты для обновления весов и смещений:

\[
W^{(l)} := W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}
\]
\[
b^{(l)} := b^{(l)} - \eta \frac{\partial L}{\partial b^{(l)}}
\]

---

## **4. Пример на практике**

### **4.1. Простая нейронная сеть**
Рассмотрим простую нейронную сеть с одним скрытым слоем и функцией активации ReLU.

#### **Архитектура сети:**
- Входной слой: 2 нейрона.
- Скрытый слой: 2 нейрона с функцией активации ReLU.
- Выходной слой: 1 нейрон с функцией активации сигмоиды.

#### **Обучение:**
1. Инициализируйте веса и смещения случайными значениями.
2. Выполните прямое распространение, чтобы получить предсказание.
3. Вычислите функцию потерь (например, бинарную кросс-энтропию).
4. Выполните обратное распространение, чтобы вычислить градиенты.
5. Обновите веса и смещения с помощью градиентного спуска.

### **4.2. Пример кода на Python**
```python
import numpy as np

# Пример данных
X = np.array([[0.1, 0.2], [0.3, 0.4]])  # Входные данные
y = np.array([0, 1])                     # Целевые значения

# Инициализация параметров
np.random.seed(0)
W1 = np.random.rand(2, 2)    # Веса первого слоя
b1 = np.zeros((1, 2))        # Смещения первого слоя
W2 = np.random.rand(2, 1)    # Веса второго слоя
b2 = np.zeros((1, 1))        # Смещения второго слоя

# Функция активации ReLU и её производная
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

# Функция активации сигмоиды и её производная
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)

# Прямое распространение
def forward_pass(X, W1, b1, W2, b2):
    z1 = np.dot(X, W1) + b1
    a1 = relu(z1)
    z2 = np.dot(a1, W2) + b2
    a2 = sigmoid(z2)
    return z1, a1, z2, a2

# Обратное распространение
def backward_pass(X, y, z1, a1, z2, a2, W1, W2):
    m = X.shape[0]
    
    # Вычисление градиентов на выходном слое
    dz2 = a2 - y.reshape(-1, 1)
    dW2 = np.dot(a1.T, dz2) / m
    db2 = np.sum(dz2, axis=0, keepdims=True) / m
    
    # Вычисление градиентов на скрытом слое
    da1 = np.dot(dz2, W2.T)
    dz1 = da1 * relu_derivative(z1)
    dW1 = np.dot(X.T, dz1) / m
    db1 = np.sum(dz1, axis=0, keepdims=True) / m
    
    return dW1, db1, dW2, db2

# Обучение
learning_rate = 0.01
for epoch in range(1000):
    # Прямое распространение
    z1, a1, z2, a2 = forward_pass(X, W1, b1, W2, b2)
    
    # Вычисление функции потерь (binary cross-entropy)
    loss = -np.mean(y * np.log(a2) + (1 - y) * np.log(1 - a2))
    
    # Обратное распространение
    dW1, db1, dW2, db2 = backward_pass(X, y, z1, a1, z2, a2, W1, W2)
    
    # Обновление весов и смещений
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {loss}")
```

---

## **5. Заключение**

Обратное распространение ошибки — это мощный инструмент для обучения нейронных сетей. Он позволяет эффективно вычислять градиенты функции потерь по всем весам сети, что делает возможным использование градиентного спуска для их оптимизации. Понимание этого метода важно для успешного применения нейронных сетей в различных задачах машинного обучения.